#!/usr/bin/env python3
"""
è¯Šæ–­arXivæœç´¢é—®é¢˜çš„æµ‹è¯•è„šæœ¬
æ£€æŸ¥é…ç½®åŠ è½½ã€æœç´¢è¯ä½¿ç”¨å’Œç½‘ç»œè¯·æ±‚
"""

import sys
import os
import yaml
import json

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from utils.logger import setup_logger
from crawlers.arxiv_crawler import ArxivCrawler

def test_config_loading():
    """æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½"""
    print("ğŸ” æ­¥éª¤1: æµ‹è¯•é…ç½®æ–‡ä»¶åŠ è½½")
    print("=" * 50)
    
    try:
        # åŠ è½½sourcesé…ç½®
        with open('config/sources.yaml', 'r', encoding='utf-8') as f:
            sources_config = yaml.safe_load(f)
        
        print("âœ… sources.yaml åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥data_sourcesç»“æ„
        if 'data_sources' in sources_config:
            print(f"ğŸ“Š å‘ç°data_sourcesé…ç½®")
            data_sources = sources_config['data_sources']
            
            # æ£€æŸ¥arxivé…ç½®
            if 'arxiv' in data_sources:
                arxiv_config = data_sources['arxiv']
                print(f"ğŸ“Š arXivé…ç½®ä¿¡æ¯:")
                print(f"   - enabled: {arxiv_config.get('enabled', 'N/A')}")
                print(f"   - base_url: {arxiv_config.get('base_url', 'N/A')}")
                print(f"   - max_results: {arxiv_config.get('max_results', 'N/A')}")
                
                # æ£€æŸ¥æœç´¢è¯
                if 'search_terms' in arxiv_config:
                    search_terms = arxiv_config['search_terms']
                    print(f"   - æœç´¢è¯æ•°é‡: {len(search_terms)}")
                    print(f"ğŸ“ æœç´¢è¯åˆ—è¡¨:")
                    for i, term in enumerate(search_terms, 1):
                        print(f"   {i:2d}. {term}")
                        if i == 5:  # åªæ˜¾ç¤ºå‰5ä¸ª
                            print(f"   ... è¿˜æœ‰ {len(search_terms) - 5} ä¸ªæœç´¢è¯")
                            break
                else:
                    print("âŒ æœªæ‰¾åˆ°search_termsé…ç½®")
                    return False
            else:
                print("âŒ æœªæ‰¾åˆ°arxivé…ç½®")
                return False
        else:
            print("âŒ æœªæ‰¾åˆ°data_sourcesé…ç½®")
            return False
            
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False

def test_crawler_initialization():
    """æµ‹è¯•çˆ¬è™«åˆå§‹åŒ–"""
    print("\nğŸ” æ­¥éª¤2: æµ‹è¯•çˆ¬è™«åˆå§‹åŒ–")
    print("=" * 50)
    
    try:
        # è®¾ç½®è°ƒè¯•æ—¥å¿—
        logger = setup_logger("test_crawler", "DEBUG", "logs/test_arxiv.log")
        
        # åŠ è½½é…ç½®
        with open('config/sources.yaml', 'r', encoding='utf-8') as f:
            sources_config = yaml.safe_load(f)
        
        arxiv_config = sources_config['data_sources']['arxiv']
        print(f"ğŸ“Š ä¼ å…¥çˆ¬è™«çš„é…ç½®:")
        print(f"   {json.dumps(arxiv_config, indent=2, ensure_ascii=False)}")
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = ArxivCrawler(arxiv_config)
        
        print(f"âœ… çˆ¬è™«åˆå§‹åŒ–æˆåŠŸ")
        print(f"ğŸ“Š çˆ¬è™«å±æ€§:")
        print(f"   - base_url: {crawler.base_url}")
        print(f"   - config: {crawler.config}")
        
        return crawler
        
    except Exception as e:
        print(f"âŒ çˆ¬è™«åˆå§‹åŒ–å¤±è´¥: {e}")
        return None

def test_search_terms_usage(crawler):
    """æµ‹è¯•æœç´¢è¯çš„ä½¿ç”¨"""
    print("\nğŸ” æ­¥éª¤3: æµ‹è¯•æœç´¢è¯ä½¿ç”¨")
    print("=" * 50)
    
    if not crawler:
        print("âŒ çˆ¬è™«æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    try:
        # æ£€æŸ¥é…ç½®ä¸­çš„æœç´¢è¯
        search_terms = crawler.config.get('search_terms', [])
        max_results = crawler.config.get('max_results', 50)
        
        print(f"ğŸ“Š çˆ¬è™«é…ç½®ä¸­çš„æœç´¢è¯:")
        print(f"   - æœç´¢è¯æ•°é‡: {len(search_terms)}")
        print(f"   - æ¯æ¬¡æœ€å¤§ç»“æœæ•°: {max_results}")
        
        if not search_terms:
            print("âŒ æœç´¢è¯åˆ—è¡¨ä¸ºç©º")
            return False
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæœç´¢è¯
        print(f"ğŸ“ å°†è¦ä½¿ç”¨çš„æœç´¢è¯:")
        for i, term in enumerate(search_terms[:3], 1):
            print(f"   {i}. {term}")
        
        # æµ‹è¯•å•ä¸ªæœç´¢è¯ï¼ˆä¸å®é™…å‘é€è¯·æ±‚ï¼‰
        test_query = search_terms[0]
        print(f"\nğŸ§ª æµ‹è¯•æœç´¢è¯: {test_query}")
        print(f"ğŸ“Š è¿™ä¸ªæœç´¢è¯åº”è¯¥äº§ç”Ÿä»¥ä¸‹è¯·æ±‚:")
        print(f"   - URL: {crawler.base_url}")
        print(f"   - å‚æ•°: search_query='{test_query}', max_results={max_results}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æœç´¢è¯æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_actual_search(crawler, dry_run=True):
    """æµ‹è¯•å®é™…æœç´¢ï¼ˆå¯é€‰æ‹©æ˜¯å¦çœŸå®å‘é€è¯·æ±‚ï¼‰"""
    print(f"\nğŸ” æ­¥éª¤4: æµ‹è¯•å®é™…æœç´¢ ({'é¢„è§ˆæ¨¡å¼' if dry_run else 'çœŸå®è¯·æ±‚'})")
    print("=" * 50)
    
    if not crawler:
        print("âŒ çˆ¬è™«æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    try:
        search_terms = crawler.config.get('search_terms', [])
        if not search_terms:
            print("âŒ æ²¡æœ‰æœç´¢è¯å¯ç”¨")
            return False
        
        if dry_run:
            print("ğŸ”„ é¢„è§ˆæ¨¡å¼ï¼šæ¨¡æ‹Ÿè¯·æ±‚å‘é€è¿‡ç¨‹")
            for i, query in enumerate(search_terms[:2], 1):  # åªæµ‹è¯•å‰2ä¸ª
                print(f"\n   ğŸ“¤ æ¨¡æ‹Ÿè¯·æ±‚ {i}:")
                print(f"      æœç´¢è¯: {query}")
                print(f"      URL: {crawler.base_url}")
                print(f"      å‚æ•°: search_query='{query}', max_results=3")
                print(f"      çŠ¶æ€: ğŸŸ¡ å·²å‡†å¤‡å‘é€ï¼ˆé¢„è§ˆæ¨¡å¼è·³è¿‡ï¼‰")
        else:
            print("ğŸ”„ çœŸå®æ¨¡å¼ï¼šå‘é€å®é™…è¯·æ±‚")
            # åªä½¿ç”¨ç¬¬ä¸€ä¸ªæœç´¢è¯è¿›è¡Œæµ‹è¯•
            test_query = search_terms[0]
            print(f"   ğŸ“¤ å‘é€çœŸå®è¯·æ±‚: {test_query}")
            
            # è®¾ç½®å°çš„ç»“æœæ•°é‡ä»¥åŠ å¿«æµ‹è¯•
            papers = crawler.search_papers(test_query, max_results=3)
            print(f"   ğŸ“¥ æ”¶åˆ°å“åº”: {len(papers)} ç¯‡è®ºæ–‡")
            
            if papers:
                print(f"   ğŸ“ ç¤ºä¾‹è®ºæ–‡:")
                for i, paper in enumerate(papers[:2], 1):
                    print(f"      {i}. {paper.title[:60]}...")
            else:
                print(f"   âš ï¸ æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
        
        return True
        
    except Exception as e:
        print(f"âŒ æœç´¢æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_recent_papers_method(crawler, days=1):
    """æµ‹è¯•get_recent_papersæ–¹æ³•"""
    print(f"\nğŸ” æ­¥éª¤5: æµ‹è¯•get_recent_papersæ–¹æ³•")
    print("=" * 50)
    
    if not crawler:
        print("âŒ çˆ¬è™«æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    try:
        print(f"ğŸ”„ è°ƒç”¨get_recent_papers(days={days})...")
        
        # æ£€æŸ¥æ–¹æ³•æ˜¯å¦ä¼šä½¿ç”¨æ‰€æœ‰æœç´¢è¯
        search_terms = crawler.config.get('search_terms', [])
        print(f"ğŸ“Š é¢„æœŸè¡Œä¸º:")
        print(f"   - å°†éå† {len(search_terms)} ä¸ªæœç´¢è¯")
        print(f"   - æ¯ä¸ªæœç´¢è¯æœ€å¤šè·å– {crawler.config.get('max_results', 50)} ç¯‡è®ºæ–‡")
        print(f"   - è¿‡æ»¤æœ€è¿‘ {days} å¤©çš„è®ºæ–‡")
        
        # å®é™…è°ƒç”¨ï¼ˆä½¿ç”¨è¾ƒå°‘çš„æœç´¢è¯è¿›è¡Œæµ‹è¯•ï¼‰
        print(f"\nğŸ§ª æ‰§è¡Œæµ‹è¯•è°ƒç”¨...")
        
        # ä¸´æ—¶ä¿®æ”¹é…ç½®ï¼Œåªä½¿ç”¨å‰2ä¸ªæœç´¢è¯è¿›è¡Œæµ‹è¯•
        original_search_terms = crawler.config.get('search_terms', [])
        crawler.config['search_terms'] = original_search_terms[:2]  # åªç”¨å‰2ä¸ª
        crawler.config['max_results'] = 3  # é™åˆ¶ç»“æœæ•°é‡
        
        papers = crawler.get_recent_papers(days)
        
        # æ¢å¤åŸé…ç½®
        crawler.config['search_terms'] = original_search_terms
        
        print(f"âœ… æ–¹æ³•è°ƒç”¨å®Œæˆ")
        print(f"ğŸ“Š ç»“æœ:")
        print(f"   - è·å¾—è®ºæ–‡æ•°é‡: {len(papers)}")
        
        if papers:
            print(f"ğŸ“ ç¤ºä¾‹è®ºæ–‡:")
            for i, paper in enumerate(papers[:3], 1):
                print(f"   {i}. {paper.title[:60]}...")
                print(f"      å‘å¸ƒæ—¥æœŸ: {paper.publish_date.strftime('%Y-%m-%d')}")
                print(f"      URL: {paper.url}")
        else:
            print(f"âš ï¸ æœªè·å¾—ä»»ä½•è®ºæ–‡")
            print(f"ğŸ’¡ å¯èƒ½åŸå› :")
            print(f"   - æœ€è¿‘{days}å¤©å†…æ²¡æœ‰ç›¸å…³è®ºæ–‡")
            print(f"   - æœç´¢è¯è¿‡äºä¸¥æ ¼")
            print(f"   - ç½‘ç»œè¿æ¥é—®é¢˜")
            print(f"   - arXiv APIé™åˆ¶")
        
        return True
        
    except Exception as e:
        print(f"âŒ get_recent_papersæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("ğŸ¯ arXivæœç´¢é—®é¢˜è¯Šæ–­å·¥å…·")
    print("=" * 80)
    print("è¿™ä¸ªè„šæœ¬å°†å¸®åŠ©è¯Šæ–­ä¸ºä»€ä¹ˆæœç´¢è¯æ²¡æœ‰è¢«æ‰¾åˆ°ä»¥åŠè¯·æ±‚æ²¡æœ‰å‘é€çš„é—®é¢˜")
    print("=" * 80)
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs("logs", exist_ok=True)
    
    # æ­¥éª¤1: æµ‹è¯•é…ç½®åŠ è½½
    config_ok = test_config_loading()
    
    if not config_ok:
        print("\nâŒ é…ç½®åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return 1
    
    # æ­¥éª¤2: æµ‹è¯•çˆ¬è™«åˆå§‹åŒ–
    crawler = test_crawler_initialization()
    
    if not crawler:
        print("\nâŒ çˆ¬è™«åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•")
        return 1
    
    # æ­¥éª¤3: æµ‹è¯•æœç´¢è¯ä½¿ç”¨
    search_ok = test_search_terms_usage(crawler)
    
    if not search_ok:
        print("\nâŒ æœç´¢è¯é…ç½®æœ‰é—®é¢˜")
        return 1
    
    # æ­¥éª¤4: æµ‹è¯•æœç´¢ï¼ˆé¢„è§ˆæ¨¡å¼ï¼‰
    search_test_ok = test_actual_search(crawler, dry_run=True)
    
    # æ­¥éª¤5: è¯¢é—®æ˜¯å¦è¿›è¡ŒçœŸå®è¯·æ±‚æµ‹è¯•
    print(f"\nâ“ æ˜¯å¦è¦è¿›è¡ŒçœŸå®çš„ç½‘ç»œè¯·æ±‚æµ‹è¯•ï¼Ÿ")
    print(f"   è¿™å°†å®é™…å‘arXivå‘é€è¯·æ±‚ï¼ˆæ•°é‡æœ‰é™ï¼‰")
    response = input("   è¾“å…¥ 'y' ç»§ç»­ï¼Œå…¶ä»–ä»»æ„é”®è·³è¿‡: ").lower().strip()
    
    if response == 'y':
        test_actual_search(crawler, dry_run=False)
        test_recent_papers_method(crawler, days=1)
    else:
        print("â­ï¸ è·³è¿‡çœŸå®è¯·æ±‚æµ‹è¯•")
    
    print("\n" + "=" * 80)
    print("ğŸ” è¯Šæ–­æ€»ç»“:")
    print(f"   1. é…ç½®åŠ è½½: {'âœ… æˆåŠŸ' if config_ok else 'âŒ å¤±è´¥'}")
    print(f"   2. çˆ¬è™«åˆå§‹åŒ–: {'âœ… æˆåŠŸ' if crawler else 'âŒ å¤±è´¥'}")
    print(f"   3. æœç´¢è¯é…ç½®: {'âœ… æ­£å¸¸' if search_ok else 'âŒ å¼‚å¸¸'}")
    print(f"   4. æœç´¢æµç¨‹: {'âœ… æ­£å¸¸' if search_test_ok else 'âŒ å¼‚å¸¸'}")
    
    if config_ok and crawler and search_ok and search_test_ok:
        print("\nğŸ’¡ å»ºè®®:")
        print("   é…ç½®å’Œä»£ç çœ‹èµ·æ¥éƒ½æ­£å¸¸ã€‚å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°è®ºæ–‡ï¼Œå¯èƒ½æ˜¯å› ä¸º:")
        print("   â€¢ æœ€è¿‘å‡ å¤©ç¡®å®æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„æ–°è®ºæ–‡")
        print("   â€¢ æœç´¢è¯è¿‡äºä¸¥æ ¼ï¼Œå»ºè®®å°è¯•æ›´å®½æ³›çš„å…³é”®è¯")
        print("   â€¢ ç½‘ç»œè¿æ¥é—®é¢˜æˆ–arXivæœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
        print("   â€¢ å°è¯•å¢åŠ å¤©æ•°å‚æ•°: --days 7")
    else:
        print("\nâŒ å‘ç°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°å¤±è´¥çš„æ­¥éª¤")
    
    print("=" * 80)
    return 0

if __name__ == "__main__":
    sys.exit(main())